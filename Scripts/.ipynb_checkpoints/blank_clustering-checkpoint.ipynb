{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering python notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cex\\\\Documents\\\\Smart Cities and Urban Analytics\\\\Term 2\\\\SDC\\\\Assessment'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries required\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# run this command too - just to allow more data to be displayed than default\n",
    "pd.set_option('display.max_rows', 200)\n",
    "# this one ensures graphs properly display in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the filepath\n",
    "file_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the data into a dataframe\n",
    "data = pd.read_csv()\n",
    "#check that is has been read correctly\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to merge by origin\n",
    "msoa = data.groupby(\"Area of residence\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to automatically scale the dataframe\n",
    "\n",
    "#this takes arguments: X ,axis = 0, with_mean = True, with_std = True, copy = True\n",
    "#X is the data to centre and scale\n",
    "#axs: int (0 by default) - axis used to compute means and Std along, if 0  independentky\n",
    "#standardise each feature, other (if 1) standardize each sample\n",
    "#with_mean: boolean, True by default, if true centre the data before scaling\n",
    "#with_std: boolean, True by default, if True scale the data to unit variance\n",
    "#copy: boolean, optional, default True, set to False to perform inplace row normalisation and avoid a copy\n",
    "\n",
    "#NaNs are treated as missing values: disregarded\n",
    "\n",
    "scaled = preprocessing.scale(transport_percentage) # add your dataframe name here\n",
    "\n",
    "#this creates a numpy array\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change the numpy array back into a dataframe\n",
    "scaled_df = pd.DataFrame({'Private_transport': scaled[:, 0], 'Public _transport': scaled[:, 1], 'People_power':scaled[:, 2]})\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DBSCAN cluster object\n",
    "\n",
    "#eps is the maximum distance between two samples for one to be considered as in the neighbourhood of other other\n",
    "#min samples is the number of samples within that distance to make that point a core point, this includes the point itself\n",
    "#this also takes several other arguments e.g.\n",
    "#metric= 'euclidean', metric_params=None, algorith='auto', leaf_szie=30, p = None, n_jobs=None\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5) \n",
    "\n",
    "#algorith to be used by the NearestNeighbors module to compute pointwise distance and find neighest neighbours - see NN module documentation\n",
    "# run the .fit() function on the scaled dataset\n",
    "dbscan.fit(scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the labels from the clusters\n",
    "dbscan_labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the silhouette score\n",
    "metrics.silhouette_score(scaled, dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the labels to the original dataframe\n",
    "transport_percentage=transport_percentage.assign(label = dbscan_labels)\n",
    "\n",
    "#and check the calue counts\n",
    "transport_percentage.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of clusters\n",
    "n_clusters_ = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "#check the number of points outside the cluster\n",
    "n_noise_ = list(dbscan_labels).count(-1)\n",
    "\n",
    "#print the number of clusters\n",
    "print(n_clusters_)\n",
    "#print the number of points outside the cluster\n",
    "print(n_noise_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of clusters to explore\n",
    "k_cluster = 5\n",
    "#set the random seed\n",
    "random_seed = 1\n",
    "#random state: int, default = None\n",
    "#determines random number generation for ecntroid intialisation, use an int to make the randomness deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the method for kmeans\n",
    "kmeans_method = KMeans(n_clusters=k_cluster,random_state=random_seed)\n",
    "#apply the fit to the scaled dataset\n",
    "kmeans_method.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the labels to original databse\n",
    "transport_percenatege_kmeans = transport_percentage.assign(label = kmeans_method.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow plot over multiple k's\n",
    "\n",
    "#calculate SSE for a range of number of cluster\n",
    "\n",
    "list_SSE = []\n",
    "min_k = 1\n",
    "max_k = 10\n",
    "range_k = range(min_k, max_k+1)\n",
    "for i in range_k:\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(scaled)\n",
    "    # inertia is a concept in physics. Roughly it means SSE of clustering.\n",
    "    list_SSE.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range_k, list_SSE, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silhouette score over multiple k's\n",
    "\n",
    "#empty array to hold the silhouette scores\n",
    "silhouette = []\n",
    "\n",
    "#iterate over the number of clusters from 2 to 10\n",
    "for i in range(2, 21):\n",
    "    #create an empty array to store the average\n",
    "    average = []\n",
    "    #repeat the kmeans clustering 10 times\n",
    "    for x in range(1, 10):\n",
    "        #setting the number of clusters as i\n",
    "        k_cluster = i\n",
    "        #create a random integer for the random seed\n",
    "        random_seed = random.randint(1,101)\n",
    "        #run the kmeans analysis\n",
    "        kmeans_method = KMeans(n_clusters=k_cluster,random_state=random_seed)\n",
    "        #fit it to the scaled dataset\n",
    "        kmeans_method.fit(scaled)\n",
    "        #get the labels\n",
    "        labels = kmeans_method.labels_\n",
    "        #get the silhouette score\n",
    "        a = metrics.silhouette_score(scaled, labels)\n",
    "        #append it to the average list\n",
    "        average.append(a)\n",
    "    #get the silhouette score and append it to the silhouette \n",
    "    silhouette.append(sum(average)/len(average))\n",
    "    \n",
    "#plot the silhouette score\n",
    "plt.plot(silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of clusters expected\n",
    "n_clusters = 4\n",
    "\n",
    "#get the hierarchy method\n",
    "hierarchy = AgglomerativeClustering(n_clusters = n_clusters)\n",
    "\n",
    "#fit it to the data\n",
    "hierarchy.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign labels to the original data\n",
    "transport_percentage_hierarchy = transport_percentage.assign(label = hierarchy.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_labels = hierarchy.labels_\n",
    "\n",
    "#check the silhouette score\n",
    "metrics.silhouette_score(scaled, hierarchy_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
