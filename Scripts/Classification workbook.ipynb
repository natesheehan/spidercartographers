{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of organising a classifier:\n",
    "\n",
    "1. Organise and clean the dataset\n",
    "2. Divide the datset into training and testing subsets\n",
    "3. Use the classifier to associate feature attributes within the training dataset to known classifications\n",
    "4. Test the strength of the model fit by predicting the classifications within the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 300) # specifies number of rows to show\n",
    "pd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, read in the data required for the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organising and cleaning the data includes:\n",
    "\n",
    "1. split the `data` into two datasets corresponding to predictors `X` and the response variable `y`\n",
    "2. splt the datasets into training set and testing set\n",
    "\n",
    "The data to be used here is what cluster does the MSOA belong to as given by our clustering algorithm outputs.\n",
    "\n",
    "This requires splitting the dataset into one dataset containing the attribute data known as `attributes`, and one containing the classifications known as `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = data.drop(\"labels\", axis = 1)\n",
    "y = data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have any categorical variables we can convert the attributes and classifications into numpy arrays which is required to feed into the classifier algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this could be an issue but we shall see \n",
    "attributes.to_numpy()\n",
    "y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final stage in data preprocessing involves splitting the prepared dataset into training and testing subsets. The training data will be used to create the classifier, the esting data will then be used to test the accuracy of the classification\n",
    "\n",
    "This is done using the `train_test_split` method from `scikit`, which splits the attribute and label data into training and testing subsets.\n",
    "\n",
    "This splots according to a 75:25 split, roughly in line with convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_Selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, test_a, train_lab, test_lab = train_test_split(attributes, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classification method we will use is that of a decision tree.\n",
    "\n",
    "This takes two arrays as inputs: an array X of size `[n_smaples, n_featurs]` holding tha trainig samples, and an array Y of integer values, size `[n_samples]`, holding the class lables for the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_decision_tree = DecisionTreeClassifier(random_state=RSEED)  # creates the kNN classifier, setting it to check the 60 neighbouring points\n",
    "clf_decision_tree.fit(train_d, train_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model Accuracy: {clf_decision_tree.score(train_d, train_lab)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the accuracy of the the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_decision_tree = clf_decision_tree.predict(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print (metrics.classification_report(test_lab, test_pred_decision_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained we can then plot the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "decision_tree_depth_5 = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "tree.plot_tree(test_pred_decision_tree(train_a, train_lab))\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all nodes excpet the leaf nodes (terminal nodes which are coloured) all have 5 parts (leaf nodes don't have a question because they are where the final prediction is made:\n",
    "\n",
    "1. Question asked about the data based on a value of a feature. Each is either true or false that splits the node\n",
    "2. `gini`: the gini Impurity of the node. The average weighted Gini impurity decreases as we move down the tree\n",
    "3. `samples`: the number of observations in the node\n",
    "4. `value`: the nu of samples in each class\n",
    "5. `class` the majority classification for points in the node. In the case of leaf nodes, this is the prediction for all samples in the node "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the rules in text format, which is more intuitive to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code is based on this link: https://stackoverflow.com/a/57335067/4667568\n",
    "decision_tree_depth_5 = DecisionTreeClassifier(max_depth=5)\n",
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(decision_tree_depth_5, feature_names=vec.feature_names_)\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gini impurity of a node is the probability that a randomly chosen sample in a node would be incorrectly labelled if it was labeled by the distribution of samples in the node.\n",
    "\n",
    "At each node the decision tree searches through the features for the value to split on that results in the greatest reduction in Gini impurity (an alternative for this is the infromation gain).\n",
    "\n",
    "Eventually the weighted total Gini impurity of the last layer goes to 0, so that each node is completely pure. This means that the model may be overfitting because the nodes are constructed only using training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit overfitting we can set a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses many trees.\n",
    "\n",
    "This requires you to specify `n_estimators` which specifies how many trees shsould be created in the construction of the whole forest. The more trees chosen, the longer this will take.\n",
    "\n",
    "The benefit of this is that it injects some randomness into the fitting of trees to reduce overfitting seen as part of decision trees, and hence produces an overall better model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = randomForestClassifier(n_estimators=100)\n",
    "clf.fit(train_d, train_lab)\n",
    "test_pred_random_forest = clf.predict(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(test_lab, test_pred_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with a single tree, you can inspect the structure and rules of each tree inside a random forst. if we created and trained a `RandomForestClassifier` called `rf`, we can extract the tree using `rf.estimators_[0]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
