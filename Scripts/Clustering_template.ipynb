{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used as the basic notebook to run all subsequent clustering scripts across the \n",
    "datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is used to identify groups within data that have similar underlying characteristics, while those in other clusters are seen as different. The idea is to make sure that the data points in one cluster are as similar as possible while being different compared to other clusters. For our purpose this means that we want to find MSOA that have similar transport characteristics such that they create a subgroup of MSOAs.\n",
    "\n",
    "This is an unsupervised machine learning methodology whereby we don't have already labeled data and thus the purpose of this is to create the labels. Hence, we don't have anything to compare the outcomes to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to clean the data to make sure that it is in the right format to run the algorithms below. The data that is fed in here has already gone through the transformation and standardisation process and so it is just making sure that it is in the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cex\\\\Documents\\\\Smart Cities and Urban Analytics\\\\Term 2\\\\SDC\\\\Assessment'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries required\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# run this command too - just to allow more data to be displayed than default\n",
    "pd.set_option('display.max_rows', 200)\n",
    "# this one ensures graphs properly display in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the filepath\n",
    "file_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the data into a dataframe\n",
    "data = pd.read_csv()\n",
    "#check that is has been read correctly\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to merge by origin\n",
    "msoa = data.groupby(\"Area of residence\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to automatically scale the dataframe\n",
    "\n",
    "#this takes arguments: X ,axis = 0, with_mean = True, with_std = True, copy = True\n",
    "#X is the data to centre and scale\n",
    "#axs: int (0 by default) - axis used to compute means and Std along, if 0  independentky\n",
    "#standardise each feature, other (if 1) standardize each sample\n",
    "#with_mean: boolean, True by default, if true centre the data before scaling\n",
    "#with_std: boolean, True by default, if True scale the data to unit variance\n",
    "#copy: boolean, optional, default True, set to False to perform inplace row normalisation and avoid a copy\n",
    "\n",
    "#NaNs are treated as missing values: disregarded\n",
    "\n",
    "scaled = preprocessing.scale(transport_percentage) # add your dataframe name here\n",
    "\n",
    "#this creates a numpy array\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change the numpy array back into a dataframe\n",
    "scaled_df = pd.DataFrame({'Private_transport': scaled[:, 0], 'Public _transport': scaled[:, 1], 'People_power':scaled[:, 2]})\n",
    "print(scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBScan creates a clustering of points based on local proximity. This means that it assigns points to clusters only when a certain number of points lie within a given distance threshold, often given by euclidean distance. In effect this method seperates clusters on the basis of density in terms of the number of points inthe surrounding areas.\n",
    "\n",
    "This works by starting at a random point in the dataset. If the minimum number of points as stipulated in the `min_samples` variable surroundings the point within the threshold distance then a cluster is formed. If the point does not satisfy this criteria then it is labelled as noise. This process continues throughout all points, merging clusters where they already exist such that we get the outcome of the number of clusters.\n",
    "\n",
    "The benefit of this is that it can identify outliers well, as those that are not within a given density of points, and thus does not have to cluster every single point. \n",
    "\n",
    "The issue is that this does not deal well with uniform data points or clusters with different densities, you need prior knowledge of the data to specify the distance measure and the minimum number of points, and does not do well when data has a high degree of density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DBSCAN cluster object\n",
    "\n",
    "#eps is the maximum distance between two samples for one to be considered as in the neighbourhood of other other\n",
    "#min samples is the number of samples within that distance to make that point a core point, this includes the point itself\n",
    "#this also takes several other arguments e.g.\n",
    "#metric= 'euclidean', metric_params=None, algorith='auto', leaf_szie=30, p = None, n_jobs=None\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5) \n",
    "\n",
    "#algorith to be used by the NearestNeighbors module to compute pointwise distance and find neighest neighbours - see NN module documentation\n",
    "# run the .fit() function on the scaled dataset\n",
    "dbscan.fit(scaled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the labels from the clusters\n",
    "dbscan_labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the silhouette score\n",
    "metrics.silhouette_score(scaled, dbscan_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the labels to the original dataframe\n",
    "transport_percentage=transport_percentage.assign(label = dbscan_labels)\n",
    "\n",
    "#and check the calue counts\n",
    "transport_percentage.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of clusters\n",
    "n_clusters_ = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "#check the number of points outside the cluster\n",
    "n_noise_ = list(dbscan_labels).count(-1)\n",
    "\n",
    "#print the number of clusters\n",
    "print(n_clusters_)\n",
    "#print the number of points outside the cluster\n",
    "print(n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the silhoutte score\n",
    "print(\"The Silhouette score is: %0.2f\" % metrics.silhouette_score(scaled, dbscan_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette scale can be used to measure the performance of dbscan. This is calculated using the mean intra-cluster distance between points and the mean nearest-cluster distance. This ranges from -1 to 1 with 1 being the best score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kmeans clustering algorithm works by breaking down the dataset into groups based on proximity of points. This is based intially on a predefined number of centroids randomly assigned within space. This then assigns individual points to their nearest centroid. Once this is complete, the centroid of the space is recalculted and repositioned such that it lies within the centre of the given points. Points are then reassigned based on their distance to the centroid and the process occurs iteratively until there is no more reassignment.\n",
    "\n",
    "The issue with this methodology is that it requires a knowledge of the number of clusters (although this can be solved through the eblow methods), this is sensitive to where the centroids are initially placed (although this can be solved through repeat measurements of silhouette analysis), this is sensitive to outliers and is incapable of handling clusters of non-convex shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of clusters to explore\n",
    "k_cluster = 5\n",
    "#set the random seed\n",
    "random_seed = 1\n",
    "#random state: int, default = None\n",
    "#determines random number generation for ecntroid intialisation, use an int to make the randomness deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the method for kmeans\n",
    "kmeans_method = KMeans(n_clusters=k_cluster,random_state=random_seed)\n",
    "#apply the fit to the scaled dataset\n",
    "kmeans_method.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign the labels to original databse\n",
    "transport_percenatege_kmeans = transport_percentage.assign(label = kmeans_method.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "This method can be evaluated using:\n",
    "\n",
    "1) Elbow Method\n",
    "    - This gives us an idea of what a good number of clusters would be based on the sum of squared (SSE) distance between data points and their assigned centroids\n",
    "    - The number is picked based on where the SSE begins to flatten out and forms an elbow\n",
    "    - This can sometimes not show an elbow and therefore we can support this with Silhouette analysis\n",
    "2) Silhouette Analysis\n",
    "    - This can be used to determine how seperated clusters as explained above\n",
    "    - The idea is to make sure that the coefficients are as large as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow plot over multiple k's\n",
    "\n",
    "#calculate SSE for a range of number of cluster\n",
    "\n",
    "list_SSE = []\n",
    "min_k = 1\n",
    "max_k = 10\n",
    "range_k = range(min_k, max_k+1)\n",
    "for i in range_k:\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=300,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(scaled)\n",
    "    # inertia is a concept in physics. Roughly it means SSE of clustering.\n",
    "    list_SSE.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range_k, list_SSE, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silhouette score over multiple k's\n",
    "\n",
    "#empty array to hold the silhouette scores\n",
    "silhouette = []\n",
    "\n",
    "#iterate over the number of clusters from 2 to 10\n",
    "for i in range(2, 21):\n",
    "    #create an empty array to store the average\n",
    "    average = []\n",
    "    #repeat the kmeans clustering 10 times\n",
    "    for x in range(1, 10):\n",
    "        #setting the number of clusters as i\n",
    "        k_cluster = i\n",
    "        #create a random integer for the random seed\n",
    "        random_seed = random.randint(1,101)\n",
    "        #run the kmeans analysis\n",
    "        kmeans_method = KMeans(n_clusters=k_cluster,random_state=random_seed)\n",
    "        #fit it to the scaled dataset\n",
    "        kmeans_method.fit(scaled)\n",
    "        #get the labels\n",
    "        labels = kmeans_method.labels_\n",
    "        #get the silhouette score\n",
    "        a = metrics.silhouette_score(scaled, labels)\n",
    "        #append it to the average list\n",
    "        average.append(a)\n",
    "    #get the silhouette score and append it to the silhouette \n",
    "    silhouette.append(sum(average)/len(average))\n",
    "    \n",
    "#plot the silhouette score\n",
    "plt.plot(silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the number of clusters expected\n",
    "n_clusters = 4\n",
    "\n",
    "#get the hierarchy method\n",
    "hierarchy = AgglomerativeClustering(n_clusters = n_clusters)\n",
    "\n",
    "#fit it to the data\n",
    "hierarchy.fit(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign labels to the original data\n",
    "transport_percentage_hierarchy = transport_percentage.assign(label = hierarchy.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy_labels = hierarchy.labels_\n",
    "\n",
    "#check the silhouette score\n",
    "metrics.silhouette_score(scaled, hierarchy_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
